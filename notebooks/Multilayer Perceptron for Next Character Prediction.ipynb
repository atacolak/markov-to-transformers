{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Predicting Next Characters in Names using Trigram Neural Network**\n",
    "\n",
    "This notebook implements a simple neural network to predict the next character in a given name using a trigram-based approach. \n",
    "\n",
    "We start by processing and encoding character-level data, followed by training the neural network using PyTorch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('../data/names.txt', 'r') as f:\n",
    "    names = f.read().splitlines()\n",
    "names[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Character Encoding\n",
    "\n",
    "We create mappings from characters to integers and vice versa, which will help us encode characters numerically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "chs = {ch for name in names for ch in name}\n",
    "\n",
    "s_to_i = {ch : i for i, ch in enumerate(sorted(chs), 1)}\n",
    "s_to_i['.'] = 0 # Special end-of-name character\n",
    "\n",
    "i_to_s = {v: k for k, v in s_to_i.items()} # Reverse mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing Data\n",
    "\n",
    "Here, we extract trigram data: given two characters, we predict the third."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "xs_ch1 = [] # First character in trigram\n",
    "xs_ch2 = [] # Second character in trigram\n",
    "\n",
    "ys = [] # Next character (target)\n",
    "\n",
    "for name in names[:1]:\n",
    "    name = f\".{name}.\"\n",
    "    for ch1, ch2, ch3 in zip(name, name[1:], name[2:]):\n",
    "        xs_ch1.append(s_to_i[ch1])\n",
    "        xs_ch2.append(s_to_i[ch2])\n",
    "        ys.append(s_to_i[ch3])\n",
    "\n",
    "xs_ch1 = torch.tensor(xs_ch1)\n",
    "xs_ch2 = torch.tensor(xs_ch2)\n",
    "ys = torch.tensor(ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-Hot Encoding\n",
    "\n",
    "We encode each character numerically using one-hot encoding, creating a binary vector for each character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 54])\n",
      "torch.Size([54, 27])\n"
     ]
    }
   ],
   "source": [
    "xs_ch1_enc = F.one_hot(xs_ch1, num_classes=27).float()\n",
    "xs_ch2_enc = F.one_hot(xs_ch2, num_classes=27).float()\n",
    "\n",
    "xs_enc = torch.cat([xs_ch1_enc, xs_ch2_enc], dim=1)\n",
    "\n",
    "W = torch.randn((54, 27), requires_grad=True)\n",
    "print(xs_enc.shape)\n",
    "print(W.shape)\n",
    "\n",
    "#import matplotlib.pyplot as plt\n",
    "#plt.imshow(xs_enc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Probabilities\n",
    "\n",
    "Compute the logits (raw predictions) and then convert these into probabilities using softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = xs_enc @ W # logits, e.g. log-counts\n",
    "counts = logits.exp() # equiv. to the probability matrix in the Trigram-based Markov Models notebook\n",
    "probs = counts / counts.sum(1, keepdim=True) # normalize the counts, e.g. Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4.8170e-02, 2.3410e-03, 2.8530e-04, 1.0512e-03, 1.4925e-02, 1.0527e-02,\n",
       "         4.1374e-02, 2.7987e-02, 2.3826e-02, 1.9757e-01, 6.6870e-02, 6.9573e-02,\n",
       "         8.5822e-03, 9.0980e-03, 1.1941e-02, 4.5654e-03, 1.7216e-02, 5.3950e-03,\n",
       "         2.3046e-01, 1.3439e-02, 2.7343e-03, 3.4354e-03, 1.4182e-02, 9.3642e-02,\n",
       "         6.1889e-03, 6.7817e-02, 6.8083e-03],\n",
       "        [3.7325e-02, 3.8273e-03, 2.1493e-03, 4.0323e-03, 2.1233e-01, 2.3264e-03,\n",
       "         7.2855e-04, 6.7144e-02, 4.6694e-03, 1.0461e-02, 2.8412e-03, 4.3954e-03,\n",
       "         5.4544e-03, 1.7789e-02, 1.7657e-03, 3.4086e-02, 2.8205e-03, 2.2611e-02,\n",
       "         1.7349e-02, 5.4238e-03, 7.9163e-02, 7.1498e-02, 3.0284e-03, 6.5278e-03,\n",
       "         1.3960e-01, 2.2509e-01, 1.5561e-02],\n",
       "        [6.0847e-02, 3.3098e-02, 7.4545e-02, 1.0579e-01, 8.0880e-02, 7.7577e-03,\n",
       "         8.1237e-03, 3.7805e-02, 4.9801e-02, 4.8394e-02, 1.6298e-03, 4.3118e-03,\n",
       "         1.6220e-02, 8.3676e-02, 2.8420e-02, 2.2705e-01, 2.9415e-02, 4.9935e-03,\n",
       "         1.0020e-02, 2.8813e-03, 1.1350e-02, 1.5624e-02, 3.6580e-03, 3.3789e-03,\n",
       "         4.2000e-03, 3.2723e-02, 1.3411e-02],\n",
       "        [6.7606e-04, 3.4898e-02, 2.8097e-01, 3.0381e-01, 1.6631e-02, 1.3535e-02,\n",
       "         7.7939e-03, 6.2363e-02, 6.1974e-02, 1.8287e-02, 4.1415e-03, 4.1845e-03,\n",
       "         1.0204e-02, 2.3088e-02, 5.7659e-03, 4.3460e-02, 3.0370e-02, 7.5003e-03,\n",
       "         1.5324e-02, 8.2803e-04, 1.8754e-03, 1.9759e-03, 1.4747e-02, 1.0509e-03,\n",
       "         2.6608e-03, 3.0307e-02, 1.5726e-03]], grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Predictions\n",
    "\n",
    "Here, we demonstrate how predictions and their probabilities are calculated and measure prediction quality using negative log likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------\n",
      "Trigram 1: .e -> m\n",
      "Input to the neural net: .e\n",
      "Actual next character: 13\n",
      "Output probabilities for the next character: 0.0091\n",
      "Negative log likelihood: 4.6997\n",
      "--------------\n",
      "Trigram 2: em -> m\n",
      "Input to the neural net: em\n",
      "Actual next character: 13\n",
      "Output probabilities for the next character: 0.0178\n",
      "Negative log likelihood: 4.0292\n",
      "--------------\n",
      "Trigram 3: mm -> a\n",
      "Input to the neural net: mm\n",
      "Actual next character: 1\n",
      "Output probabilities for the next character: 0.0331\n",
      "Negative log likelihood: 3.4083\n",
      "--------------\n",
      "Trigram 4: ma -> .\n",
      "Input to the neural net: ma\n",
      "Actual next character: 0\n",
      "Output probabilities for the next character: 0.0007\n",
      "Negative log likelihood: 7.2992\n",
      "Average nll: 4.859097957611084\n"
     ]
    }
   ],
   "source": [
    "nlls = torch.zeros(4)\n",
    "\n",
    "for i in range(4):\n",
    "    ch1 = xs_ch1[i].item()\n",
    "    ch2 = xs_ch2[i].item()\n",
    "    ch3 = ys[i].item()\n",
    "    print('--------------')\n",
    "    print(f\"Trigram {i+1}: {i_to_s[ch1]}{i_to_s[ch2]} -> {i_to_s[ch3]}\")\n",
    "    print(f\"Input to the neural net: {i_to_s[ch1]}{i_to_s[ch2]}\")\n",
    "    print(f\"Actual next character: {ch3}\")\n",
    "    prob = probs[i, ch3]\n",
    "    print(f\"Output probabilities for the next character: {prob:.4f}\")\n",
    "    logprob = torch.log(prob)\n",
    "    print(f\"Negative log likelihood: {-logprob:.4f}\")\n",
    "    nlls[i] = -logprob\n",
    "print(f\"Average nll: {nlls.mean()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Neural Network\n",
    "\n",
    "Now, let's train our neural network on the full dataset of names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "196113 trigrams created.\n"
     ]
    }
   ],
   "source": [
    "xs_ch1 = [] # ch1 of bigram\n",
    "xs_ch2 = [] # ch2 of bigram\n",
    "\n",
    "ys = [] # outs\n",
    "\n",
    "for name in names:\n",
    "    name = f\".{name}.\"\n",
    "    for ch1, ch2, ch3 in zip(name, name[1:], name[2:]):\n",
    "        xs_ch1.append(s_to_i[ch1])\n",
    "        xs_ch2.append(s_to_i[ch2])\n",
    "        ys.append(s_to_i[ch3])\n",
    "\n",
    "xs_ch1 = torch.tensor(xs_ch1)\n",
    "xs_ch2 = torch.tensor(xs_ch2)\n",
    "ys = torch.tensor(ys)\n",
    "\n",
    "elems = xs_ch1.nelement()\n",
    "print(f\"{elems} trigrams created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = torch.randn((54, 27), requires_grad=True) # initialize weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained for 100 epochs.\n",
      "loss: 2.390183448791504\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "# forward pass\n",
    "    xs_ch1_enc = F.one_hot(xs_ch1, num_classes=27).float()\n",
    "    xs_ch2_enc = F.one_hot(xs_ch2, num_classes=27).float()\n",
    "\n",
    "    xs_enc = torch.cat([xs_ch1_enc, xs_ch2_enc], dim=1)\n",
    "\n",
    "    logits = xs_enc @ W # logits, e.g. log-counts\n",
    "    counts = logits.exp() # equiv. to the probability matrix in the Trigram-based Markov Models notebook\n",
    "    probs = counts / counts.sum(1, keepdim=True) # normalize the counts, e.g. Softmax\n",
    "    loss = -probs[torch.arange(elems), ys].log().mean() + 0.015 * (W**2).mean() # Negative log likelihood with regularization\n",
    "# backward pass\n",
    "    W.grad = None\n",
    "    loss.backward()\n",
    "# update\n",
    "    W.data += -10 * W.grad\n",
    "    \n",
    "print(f\"Trained for {i+1} epochs.\\nloss: {loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interactive Prediction\n",
    "\n",
    "Finally, we use the trained model interactively to generate new names character by character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name starting with 'a': arizalinnee\n",
      "Name starting with 'b': bel\n",
      "Name starting with 'c': cbem\n",
      "Name starting with 'd': dusan\n",
      "Name starting with 'e': elgy\n",
      "Name starting with 'f': felen\n",
      "Name starting with 'g': gsone\n",
      "--------\n",
      "Goodbye!\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    usr_in = input(\"Enter first character of the name: \").lower()\n",
    "    if usr_in not in chs:\n",
    "        print(\"--------\\nGoodbye!\")\n",
    "        break\n",
    "\n",
    "    word = f'.{usr_in[-1]}'\n",
    "    \n",
    "    while True:\n",
    "        xs_ch1_enc = F.one_hot(torch.tensor(s_to_i[word[-2]]), num_classes=27).float()\n",
    "        xs_ch2_enc = F.one_hot(torch.tensor(s_to_i[word[-1]]), num_classes=27).float()\n",
    "        xs_enc = torch.cat([xs_ch1_enc, xs_ch2_enc])\n",
    "\n",
    "        logits = xs_enc @ W\n",
    "        probs = F.softmax(logits, dim=0)\n",
    "\n",
    "        sample_val = torch.multinomial(probs, 1).item()\n",
    "        next_char = i_to_s[sample_val]\n",
    "\n",
    "        if next_char == '.':\n",
    "            break\n",
    "        word += next_char\n",
    "\n",
    "    print(f\"Name starting with '{word[1]}': {word[1:]}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
