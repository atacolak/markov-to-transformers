{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Character-Level Name Generation with Positional Embeddings and MLP**\n",
    "\n",
    "This notebook implements a simple multi-layer perceptron (MLP) to generate names. \n",
    "\n",
    "We train the model on a dataset of names using positional embeddings and a neural network to predict characters step by step.\n",
    "\n",
    "---\n",
    "\n",
    "We start by loading names from a file, which we will use to train our model. \n",
    "\n",
    "Each name will be tokenized into characters, and we will assign numerical indices to each unique character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 32033 names.\n"
     ]
    }
   ],
   "source": [
    "import random \n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "with open('markov-to-transformers/data/names.txt', 'r') as f:\n",
    "    names = f.read().splitlines()\n",
    "print(f\"Saved {len(names)} names.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each character is assigned a unique index, allowing us to map between characters and numbers. \n",
    "\n",
    "We also extend this set with special tokens for handling context windows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 5\n",
    "\n",
    "chrs = list({ch for name in names for ch in name})\n",
    "chrs_pos_enc = chrs + ['.' for _ in range(block_size + 1)]\n",
    "i_to_s = {i: chr for i, chr in enumerate(sorted(chrs_pos_enc))}\n",
    "s_to_i = {v: k for k, v in i_to_s.items()} # a bit hacky. len is 27."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of zero-padding, we use positional indices to provide unique context information to the model.\n",
    "\n",
    "This prevents the model from predicting start tokens incorrectly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_posemb_dataset(names):\n",
    "    ''' Builds a dataset with positional embedding for start characters. yay!'''\n",
    "    X, Y = [], []\n",
    "\n",
    "    for name in names:\n",
    "        name = f\"{name}.\"  # Append a stop token\n",
    "        context = [v for v in range(0, block_size)] # Positional encoding for each start character\n",
    "        for ch in name:\n",
    "            X.append(context)\n",
    "            Y.append(s_to_i[ch])\n",
    "            context = context[1:] + [s_to_i[ch]] # Shift context window\n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "    return X, Y\n",
    "\n",
    "# splitting data into train, validation, and test sets\n",
    "\n",
    "n1 = int(len(names) * 0.8)\n",
    "n2 = int(len(names) * 0.9)\n",
    "random.shuffle(names)\n",
    "Xtr, Ytr = build_posemb_dataset(names[:n1])\n",
    "Xval, Yval = build_posemb_dataset(names[n1:n2])\n",
    "Xte, Yte = build_posemb_dataset(names[n2:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating an Embedding Table\n",
    "\n",
    "Instead of using one-hot vectors, we map character indices to dense embeddings, which are learned during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([182625, 5]), torch.Size([32, 8]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_len = Xtr.unique().shape[0]+1\n",
    "C = torch.rand((feature_len, 8)) # 8-dimensional embeddings\n",
    "\n",
    "Xtr.shape, C.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining the Neural Network (MLP)\n",
    "\n",
    "This is a simple feedforward neural network with one hidden layer. \n",
    "\n",
    "The first layer transforms concatenated embeddings, and the second layer predicts the next character.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22188 params.\n"
     ]
    }
   ],
   "source": [
    "squashed_dims = len(C[0]) * len(Xtr[0])\n",
    "\n",
    "W1 = torch.rand((squashed_dims, 300)) # 300 neurons in the hidden layer\n",
    "b1 = torch.rand(W1.shape[1])\n",
    "W2 = torch.rand((W1.shape[1], feature_len))\n",
    "b2 = torch.rand(feature_len)\n",
    "\n",
    "params = [C, W1, b1, W2, b2]\n",
    "\n",
    "loss_i = []\n",
    "step_i = []\n",
    "\n",
    "for p in params:\n",
    "    p.requires_grad = True\n",
    "\n",
    "print(f\"{sum(p.nelement() for p in params)} params.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Loop\n",
    "\n",
    "This loop performs mini-batch gradient descent to optimize the model. \n",
    "\n",
    "The loss is computed using cross-entropy, and weights are updated using stochastic gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trained for 140000 steps.\n",
      "train loss: 2.2013587951660156\n",
      "valid loss: 2.152350902557373\n"
     ]
    }
   ],
   "source": [
    "chunk_size = 32 # mini batch size\n",
    "\n",
    "for i in range(50000):\n",
    "# forward pass\n",
    "    batch = torch.randint(0, Xtr.shape[0], (chunk_size,)) # sample batch indices\n",
    "    batch_emb = C[Xtr[batch]].view(chunk_size, -1) # get embeddings\n",
    "    acts = torch.sigmoid(batch_emb @ W1 + b1) # hidden layer with sigmoid activation\n",
    "    logits = acts @ W2 + b2 # output layer\n",
    "    loss = F.cross_entropy(logits, Ytr[batch]) # calculate loss\n",
    "\n",
    "# backward pass\n",
    "    for p in params:\n",
    "        p.grad = None   # zeroing the gradients\n",
    "    loss.backward()     # backpropagation\n",
    "\n",
    "# optimization\n",
    "    learning_rate = 0.001\n",
    "    for p in params:\n",
    "        p.data += -learning_rate * p.grad # surfing the gradient waves\n",
    "\n",
    "# track steps\n",
    "    loss_i.append(loss.item())\n",
    "    step_i.append(i)\n",
    "# evaluate the model on a separate validation set to observe generalization\n",
    "val_emb = C[[Xval]].view(-1, squashed_dims) # get embeddings for validation set\n",
    "acts = torch.sigmoid(val_emb @ W1 + b1)\n",
    "logits = acts @ W2 + b2\n",
    "valid_loss = F.cross_entropy(logits, Yval)\n",
    "\n",
    "print(f\"trained for {len(step_i)} steps.\\ntrain loss: {loss.item()}\\nvalid loss: {valid_loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once training is complete, we evaluate the model's performance on a test set. \n",
    "\n",
    "The test loss provides an estimate of how well the model generalizes to unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 2.1484696865081787\n"
     ]
    }
   ],
   "source": [
    "test_emb = C[[Xte]].view(-1, squashed_dims) # get embeddings for test set\n",
    "acts = torch.sigmoid(test_emb @ W1 + b1)\n",
    "logits = acts @ W2 + b2\n",
    "test_loss = F.cross_entropy(logits, Yte)\n",
    "print(f\"Test loss: {test_loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that training is complete, we can sample new names from the model.\n",
    "\n",
    "We start with an empty context and iteratively predict the next character.\n",
    "\n",
    "The process stops when the model predicts a stop token (`.`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "katarud\n",
      "jasxer\n",
      "iliella\n",
      "lyziah\n",
      "merri\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    context = [v for v in range(0, block_size)] # initialize context with position indices\n",
    "    word = ''\n",
    "    while True:\n",
    "        test_emb = C[context].view(-1, squashed_dims) # get embeddings for test set\n",
    "        acts = torch.sigmoid(test_emb @ W1 + b1)\n",
    "        logits = acts @ W2 + b2\n",
    "        probs = F.softmax(logits, 1)\n",
    "        pred = torch.multinomial(probs, 1).item() # sample from probability distribution\n",
    "        context = context[1:] + [pred] # shift context window\n",
    "        if i_to_s[context[-1]] == '.': # stop if end token is predicted\n",
    "            break\n",
    "        word += i_to_s[context[-1]] # append predicted character\n",
    "    print(word)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
