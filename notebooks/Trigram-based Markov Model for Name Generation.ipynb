{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Trigram-based Markov Model for Name Generation**\n",
    "\n",
    "### Introduction\n",
    "\n",
    "This notebook implements a Trigram-based Markov Model for name generation. The goal is to learn patterns from a dataset of names and use those patterns to generate new, plausible names.\n",
    "\n",
    "Specifically, the model learns probabilities based on two-letter sequences (\"bigrams\") and predicts the next character. To avoid zero-probability issues for unseen sequences, Laplace smoothing is applied."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Load and Process Dataset\n",
    "\n",
    "Load all names from the dataset `names.txt` into a list, as this allows us to process and analyze the data efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32033\n"
     ]
    }
   ],
   "source": [
    "with open('moremake/names.txt', 'r') as f:\n",
    "    names = f.read().splitlines()\n",
    "print(len(names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Building the Bigram-to-Character Probability Table\n",
    "\n",
    "\n",
    "\n",
    "We create a nested dictionary `bi_pairs` to store the occurrences of each possible character following every bigram in our dataset.\n",
    "\n",
    "We also apply Laplace smoothing to make sure that all combinations have a **non-zero** probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "bi_pairs = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "chs = {ch for name in names for ch in name}\n",
    "chs.add('.')\n",
    "chs = sorted(chs)\n",
    "\n",
    "# apply Laplace smoothing\n",
    "for ch1 in chs:\n",
    "    for ch2 in chs:\n",
    "        for ch3 in chs:\n",
    "            bi_pairs[ch1+ch2][ch3] += 1\n",
    "\n",
    "# process names into bigrams\n",
    "for name in names:\n",
    "    name = f'.{name}.'\n",
    "    for ch1, ch2, ch3 in zip(name, name[1:], name[2:]):\n",
    "        pair = ch1 + ch2 # Pair up first character and second character\n",
    "        bi_pairs[pair][ch3] += 1 # increment the value of the occurences of the character 3 after our pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape OK\n",
      "[('.', 236), ('a', 59), ('t', 33), ('n', 16), ('l', 8), ('e', 6), ('i', 5), ('b', 3), ('o', 3)]\n"
     ]
    }
   ],
   "source": [
    "# For debugging\n",
    "\n",
    "outs_of_gh = sorted(bi_pairs['gh'].items(), key=lambda x:x[1], reverse=True) # e.g. raylei`gh`, to see if our pairs are fine.\n",
    "# usually the sequence `gh` ends with a `.` in names so `.` should be the most common\n",
    "assert len(outs_of_gh) == 27 and len(bi_pairs) == 27**2, \\\n",
    "    'Shape mismatch. All bigrams and character pairs must be defined.'\n",
    "print(\"Shape OK\")\n",
    "\n",
    "print([out for out in outs_of_gh if out[1] > 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Encoding Bigram and Output Characters\n",
    "We encode each unique bigram and character into integer IDs. This numeric encoding simplifies operations like tensor manipulations and sampling.\n",
    "\n",
    "> **NB:** The reasoning behind using a generator for ID assignment instead of using dictionary length is because:\n",
    ">\n",
    "> * It is much faster for larger datasets;\n",
    ">\n",
    "> * If we were to delete keys in the future, our ID assignments wouldn't shift."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import count\n",
    "\n",
    "bigram_id_gen = count() # Generate IDs for each encoding\n",
    "output_id_gen = count()\n",
    "\n",
    "bigram_ids = {} # ID : bigram\n",
    "output_ids = {} # ID : output\n",
    "\n",
    "for ch in chs: # Encode characters into integers\n",
    "    output_ids[next(output_id_gen)] = ch\n",
    "\n",
    "for bigram in bi_pairs.items(): # also encode each bigram into an integer\n",
    "    bigram_ids[next(bigram_id_gen)] = bigram[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Convert Counts into Probabilities\n",
    "\n",
    "Next, we convert the frequency counts into probabilities.\n",
    "\n",
    "For each bigram, we create a probability distribution over the characters that follow it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bi_pairs_prob = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "for bigram, outs in bi_pairs.items():\n",
    "    for chr, count in outs.items():\n",
    "        bi_pairs_prob[bigram][chr] = count / sum(outs.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Sampling Names from the Model\n",
    "\n",
    "Using the probability distributions, we generate new names character-by-character.\n",
    "\n",
    "We start with a provided initial character and continue sampling until we reach the end token `.`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------\n",
      "Goodbye!\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    word = ''\n",
    "    chr = input(\"Enter first character of the name: \").lower()\n",
    "    if chr not in chs:\n",
    "        print(\"--------\\nGoodbye!\")\n",
    "        break\n",
    "    chr = '.' + chr[-1]\n",
    "    word = f'{chr[-1]}'\n",
    "    \n",
    "    while True:\n",
    "        chr_tensor = torch.tensor(list(bi_pairs_prob[chr].values()))\n",
    "        sample = torch.multinomial(chr_tensor, 1, replacement=True)\n",
    "        chr = chr[-1] + output_ids[sample.item()]\n",
    "        if chr[-1] == '.':\n",
    "            break\n",
    "        word += chr[-1]\n",
    "    print(word)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encode each ID as one-hot vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_enc = F.one_hot(torch.tensor(list(bigram_ids.keys())))\n",
    "output_enc = F.one_hot(torch.tensor(list(output_ids.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-hot encoding OK\n"
     ]
    }
   ],
   "source": [
    "# For debugging\n",
    "\n",
    "assert bigram_ids[torch.argmax(bigram_enc[0]).item()] == bigram_ids[list(bigram_ids.keys())[0]], \\\n",
    "\"One-hot encoding issue!\"\n",
    "print(\"One-hot encoding OK\")\n",
    "# If the first encoded bigram doesn't match the first inserted bigram, something went wrong."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
