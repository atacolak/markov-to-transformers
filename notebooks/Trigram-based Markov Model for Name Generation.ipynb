{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Trigram-based Markov Model for Name Generation**\n",
    "\n",
    "### Introduction\n",
    "\n",
    "This notebook implements a Trigram-based Markov Model for name generation. The goal is to learn patterns from a dataset of names and use those patterns to generate new, plausible names.\n",
    "\n",
    "Specifically, the model learns probabilities based on two-letter sequences (\"bigrams\") and predicts the next character. To avoid zero-probability issues for unseen sequences, Laplace smoothing is applied."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Load and Process Dataset\n",
    "\n",
    "Load all names from the dataset `names.txt` into a list, as this allows us to process and analyze the data efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-18T04:57:54.306785Z",
     "iopub.status.busy": "2025-03-18T04:57:54.306283Z",
     "iopub.status.idle": "2025-03-18T04:57:54.319966Z",
     "shell.execute_reply": "2025-03-18T04:57:54.318638Z",
     "shell.execute_reply.started": "2025-03-18T04:57:54.306739Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32033\n"
     ]
    }
   ],
   "source": [
    "with open('ngram-to-transformers/data/names.txt', 'r') as f:\n",
    "    names = f.read().splitlines()\n",
    "print(len(names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Building the Bigram-to-Character Probability Table\n",
    "\n",
    "\n",
    "\n",
    "We create a nested dictionary `bi_pairs` to store the occurrences of each possible character following every bigram in our dataset.\n",
    "\n",
    "We also apply Laplace smoothing to make sure that all combinations have a **non-zero** probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-18T04:57:54.321948Z",
     "iopub.status.busy": "2025-03-18T04:57:54.321485Z",
     "iopub.status.idle": "2025-03-18T04:57:54.453057Z",
     "shell.execute_reply": "2025-03-18T04:57:54.451829Z",
     "shell.execute_reply.started": "2025-03-18T04:57:54.321906Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "bi_pairs = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "chs = {ch for name in names for ch in name}\n",
    "chs.add('.')\n",
    "chs = sorted(chs)\n",
    "\n",
    "# apply Laplace smoothing\n",
    "for ch1 in chs:\n",
    "    for ch2 in chs:\n",
    "        for ch3 in chs:\n",
    "            bi_pairs[ch1+ch2][ch3] += 1\n",
    "\n",
    "# process names into bigrams\n",
    "for name in names:\n",
    "    name = f'.{name}.'\n",
    "    for ch1, ch2, ch3 in zip(name, name[1:], name[2:]):\n",
    "        pair = ch1 + ch2 # Pair up first character and second character\n",
    "        bi_pairs[pair][ch3] += 1 # increment the value of the occurences of the character 3 after our pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-18T04:57:54.455646Z",
     "iopub.status.busy": "2025-03-18T04:57:54.455223Z",
     "iopub.status.idle": "2025-03-18T04:57:54.462255Z",
     "shell.execute_reply": "2025-03-18T04:57:54.461169Z",
     "shell.execute_reply.started": "2025-03-18T04:57:54.455603Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape OK\n",
      "[('.', 236), ('a', 59), ('t', 33), ('n', 16), ('l', 8), ('e', 6), ('i', 5), ('b', 3), ('o', 3)]\n"
     ]
    }
   ],
   "source": [
    "# For debugging\n",
    "\n",
    "outs_of_gh = sorted(bi_pairs['gh'].items(), key=lambda x:x[1], reverse=True) # e.g. raylei`gh`, to see if our pairs are fine.\n",
    "# usually the sequence `gh` ends with a `.` in names so `.` should be the most common\n",
    "assert len(outs_of_gh) == 27 and len(bi_pairs) == 27**2, \\\n",
    "    'Shape mismatch. All bigrams and character pairs must be defined.'\n",
    "print(\"Shape OK\")\n",
    "\n",
    "print([out for out in outs_of_gh if out[1] > 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Encoding Bigram and Output Characters\n",
    "We encode each unique bigram and character into integer IDs. This numeric encoding simplifies operations like tensor manipulations and sampling.\n",
    "\n",
    "> **NB:** The reasoning behind using a generator for ID assignment instead of using dictionary length is because:\n",
    ">\n",
    "> * It is much faster for larger datasets;\n",
    ">\n",
    "> * If we were to delete keys in the future, our ID assignments wouldn't shift."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-18T04:57:54.463907Z",
     "iopub.status.busy": "2025-03-18T04:57:54.463539Z",
     "iopub.status.idle": "2025-03-18T04:57:54.499428Z",
     "shell.execute_reply": "2025-03-18T04:57:54.498295Z",
     "shell.execute_reply.started": "2025-03-18T04:57:54.463867Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from itertools import count\n",
    "\n",
    "bigram_id_gen = count() # Generate IDs for each encoding\n",
    "output_id_gen = count()\n",
    "\n",
    "id_to_bigram = {} # ID : bigram\n",
    "id_to_out = {} # ID : output\n",
    "\n",
    "for ch in chs: # Encode characters into integers\n",
    "    id_to_out[next(output_id_gen)] = ch\n",
    "\n",
    "for bigram in bi_pairs.items(): # also encode each bigram into an integer\n",
    "    id_to_bigram[next(bigram_id_gen)] = bigram[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Convert Counts into Probabilities\n",
    "\n",
    "Next, we convert the frequency counts into probabilities.\n",
    "\n",
    "For each bigram, we create a probability distribution over the characters that follow it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-18T04:57:54.502126Z",
     "iopub.status.busy": "2025-03-18T04:57:54.501706Z",
     "iopub.status.idle": "2025-03-18T04:57:54.588848Z",
     "shell.execute_reply": "2025-03-18T04:57:54.587703Z",
     "shell.execute_reply.started": "2025-03-18T04:57:54.502082Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "bi_pairs_prob = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "for bigram, outs in bi_pairs.items():\n",
    "    for chr, count in outs.items():\n",
    "        bi_pairs_prob[bigram][chr] = count / sum(outs.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Sampling Names from the Model\n",
    "\n",
    "Using the probability distributions, we generate new names character-by-character.\n",
    "\n",
    "We start with a provided initial character and continue sampling until we reach the end token `.`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-18T04:57:54.590426Z",
     "iopub.status.busy": "2025-03-18T04:57:54.590170Z",
     "iopub.status.idle": "2025-03-18T04:57:56.542228Z",
     "shell.execute_reply": "2025-03-18T04:57:56.541163Z",
     "shell.execute_reply.started": "2025-03-18T04:57:54.590404Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-18T04:57:56.543867Z",
     "iopub.status.busy": "2025-03-18T04:57:56.543310Z",
     "iopub.status.idle": "2025-03-18T04:58:33.971339Z",
     "shell.execute_reply": "2025-03-18T04:58:33.970482Z",
     "shell.execute_reply.started": "2025-03-18T04:57:56.543830Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name starting with a: adalyn\n",
      "Name starting with b: bpubyrichazilakyn\n",
      "Name starting with c: carleighla\n",
      "Name starting with d: diryne\n",
      "Name starting with e: emarmon\n",
      "Name starting with f: frehlon\n",
      "Name starting with g: grambristinlen\n",
      "--------\n",
      "Goodbye!\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    word = ''\n",
    "    chr = input(\"Enter first character of the name: \").lower()\n",
    "    if chr not in chs:\n",
    "        print(\"--------\\nGoodbye!\")\n",
    "        break\n",
    "    chr = '.' + chr[-1]\n",
    "    word = f'{chr[-1]}'\n",
    "    \n",
    "    while True:\n",
    "        chr_tensor = torch.tensor(list(bi_pairs_prob[chr].values()))\n",
    "        sample = torch.multinomial(chr_tensor, 1, replacement=True)\n",
    "        chr = chr[-1] + id_to_out[sample.item()]\n",
    "        if chr[-1] == '.':\n",
    "            break\n",
    "        word += chr[-1]\n",
    "    print(f\"Name starting with {word[0]}: {word}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating the loss of the Markov Model\n",
    "\n",
    "This section computes the negative log-likelihood (NLL) loss for our trigram-based Markov model.  \n",
    "\n",
    "We first construct a probability matrix where rows represent bigrams and columns represent possible next characters.  \n",
    "\n",
    "Then, for each name in the dataset, we extract bigram-character sequences and evaluate the model's predicted probabilities.  \n",
    "\n",
    "The final loss value provides a measure of how well our learned probabilities match the real distribution of names.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.092747449874878\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "bigram_to_id = {v: k for k, v in id_to_bigram.items()} # Bigram : ID\n",
    "char_to_id = {v: k for k, v in id_to_out.items()} # Out : ID\n",
    "prob_matrix = torch.zeros((len(bigram_to_id), len(char_to_id)))\n",
    "#print(prob_matrix.shape)\n",
    "\n",
    "for bigram, char_probs in bi_pairs_prob.items():\n",
    "    for char, prob in char_probs.items():\n",
    "        prob_matrix[bigram_to_id[bigram], char_to_id[char]] = prob\n",
    "\n",
    "nlls = []\n",
    "\n",
    "for name in names:\n",
    "    name = f'.{name}.'\n",
    "    for ch1, ch2, ch3 in zip(name, name[1:], name[2:]):\n",
    "        bigram = ch1 + ch2\n",
    "        target_i = char_to_id[ch3]\n",
    "        bigram_i = bigram_to_id[bigram]\n",
    "        log = torch.log(prob_matrix[bigram_i])\n",
    "        nlls.append(F.nll_loss(log, torch.tensor(target_i)))\n",
    "        \n",
    "loss = sum(nlls) / len(nlls)\n",
    "print(f\"loss: {loss}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 5349227,
     "sourceId": 8896547,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
