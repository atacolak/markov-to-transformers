{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Turkish Name Generator — From Scratch\n",
        "\n",
        "This notebook builds a **character-level name generator** trained on a corpus of Turkish names.  \n",
        "We construct everything from **manual layers** to **custom BatchNorm**, train it using gradient descent,  \n",
        "and even implement **BatchNorm folding** to optimize inference speed.\n",
        "\n",
        "### Features:\n",
        "- Hand-written `Linear`, `BatchNorm1d`, `ReLU`, and `Softmax`\n",
        "- Character embedding layer\n",
        "- Tokenization and preprocessing for UTF-8-safe Turkish names\n",
        "- Training loop with manual parameter updates\n",
        "- BatchNorm layer folding into Linear layers for fast inference\n",
        "- Interactive name generation\n",
        "\n",
        "> The goal is not just to build a model...  \n",
        "> but to understand the gears and levers behind every activation.\n",
        "\n",
        "---\n",
        "\n",
        "> Built from *first principles™*, with love for the stack and respect for the flow.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Import Libraries & Set Random Seed\n",
        "Initialize the environment, import required libraries, and fix the randomness for reproducibility.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "GbLzfsHKE9ja"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "\n",
        "g = torch.Generator().manual_seed(1337)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define Model Components\n",
        "Implement the basic building blocks of a neural network:\n",
        "- Linear layers\n",
        "- Activation functions (ReLU, Softmax)\n",
        "- Custom BatchNorm1d layer with train/test behavior\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F1eFZB7cFF4n"
      },
      "outputs": [],
      "source": [
        "class Linear:\n",
        "  def __init__(self, ins, outs, bias=False):\n",
        "    self.weights = torch.empty(outs, ins)\n",
        "    torch.nn.init.kaiming_uniform_(self.weights, mode='fan_in', nonlinearity='relu', generator=g)\n",
        "    if bias:\n",
        "      self.biases = torch.rand(outs, generator=g)\n",
        "    else:\n",
        "      self.biases = None\n",
        "\n",
        "  def __call__(self, x):\n",
        "    pre_act = x @ self.weights.T\n",
        "    if self.biases is not None:\n",
        "      pre_act += self.biases\n",
        "    return pre_act\n",
        "\n",
        "  def params(self):\n",
        "    return [self.weights] + [self.biases] if self.biases is not None else [self.weights]\n",
        "  \n",
        "class Relu:\n",
        "  def __call__(self, x):\n",
        "    self.out = F.relu(x)\n",
        "    return self.out\n",
        "\n",
        "class Softmax:\n",
        "  def __call__(self, x):\n",
        "    self.out = F.softmax(x)\n",
        "    return self.out\n",
        "  \n",
        "class BatchNorm1d:\n",
        "  def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
        "    self.dim = dim\n",
        "    self.eps = eps\n",
        "    self.momentum = momentum\n",
        "    self.training = True\n",
        "    # gamma & beta: trainable parameters to scale or move the normed batch\n",
        "    self.gamma = torch.ones(dim)\n",
        "    self.beta = torch.zeros(dim)\n",
        "    # for inference\n",
        "    self.running_mean = torch.zeros(dim)\n",
        "    self.running_var = torch.ones(dim)\n",
        "\n",
        "  def __call__(self, x):\n",
        "    if self.training:   # train vars\n",
        "      xmean = x.mean(0, keepdim=True)\n",
        "      xvar = x.var(0, keepdim=True)\n",
        "    else:               # inference vars\n",
        "      xmean = self.running_mean\n",
        "      xvar = self.running_var\n",
        "\n",
        "    xhat = (x - xmean) / torch.sqrt(xvar + self.eps)\n",
        "    self.out = self.gamma * xhat + self.beta\n",
        "\n",
        "    if self.training: # calculate running mean/var\n",
        "      self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * xmean\n",
        "      self.running_var = (1 - self.momentum) * self.running_var + self.momentum * xvar\n",
        "    return self.out\n",
        "\n",
        "  def params(self):\n",
        "    return [self.gamma] + [self.beta]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PgTmKI5EW0Zm"
      },
      "source": [
        "## Load & Preprocess Turkish Names\n",
        "- Load raw Turkish names\n",
        "- Normalize characters using custom UTF-8 mapping for Turkish phonemes\n",
        "- Prepare a clean dataset of lowercase, ASCII-safe names\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vV21dz9mmszn",
        "outputId": "5439c8a8-1851-4278-c096-15025251b7b8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['jale',\n",
              " 'ali',\n",
              " 'mahmut',\n",
              " 'mansur',\n",
              " 'k#r$ad',\n",
              " 'gamze',\n",
              " 'mira^',\n",
              " 'y#cel',\n",
              " 'kubilay',\n",
              " 'hayati']"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "with open('turkish_names.txt', 'r') as f:\n",
        "  lines = f.read().splitlines()\n",
        "len(lines)\n",
        "\n",
        "trchr_to_utf8 = {\n",
        "    'İ': 'i',\n",
        "    'I': '!',\n",
        "    'Ö': '@',\n",
        "    'Ü': '#',\n",
        "    'Ş': '$',\n",
        "    'Ç': '^',\n",
        "    'Ğ': '&'}\n",
        "\n",
        "utf8_to_trchr = {v: k for k, v in trchr_to_utf8.items()}\n",
        "def undo_utf8(name: str):\n",
        "  name = ''.join(utf8_to_trchr.get(ch, ch) for ch in name).lower()\n",
        "  return name\n",
        "\n",
        "words = [word for line in lines for word in line.split()]\n",
        "names = [''.join(trchr_to_utf8.get(ch, ch) for ch in name).lower() for name in words]\n",
        "# ^ turkish phonemes transmogrified into utf-8 sigils ^-^\n",
        "names[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Character-Level Tokenization\n",
        "- Extract character vocabulary\n",
        "- Map characters to integers and vice versa\n",
        "- Prepare input-output pairs using a sliding window of context\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "m9laLxgZmuoG"
      },
      "outputs": [],
      "source": [
        "chars = ['.'] + sorted(list(set(ch for name in names for ch in name)))\n",
        "# ^ no need to sort tbh.    '.' as special start/end token.\n",
        "i_to_s = {i: chars[i] for i in range(len(chars))}\n",
        "s_to_i = {v: k for k, v in i_to_s.items()}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create Training, Validation & Test Splits\n",
        "- Shuffle dataset\n",
        "- Split into train/val/test with ratios 80/10/10\n",
        "- Convert names into tensorized (X, Y) samples\n",
        "- Verify dataset consistency via average name length check\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "PuqA7JObulfn"
      },
      "outputs": [],
      "source": [
        "block_size = 3 # how many characters should it take to predict the next one?\n",
        "# . . . -> a; . . a -> t; . a t -> a\n",
        "def create_dataset(names):\n",
        "  X, Y = [], []\n",
        "  for name in names:\n",
        "    name = f\"{name}.\"   # add end token to each name\n",
        "    context = [0] * block_size # initialized as `. . .`\n",
        "    for ch in name:\n",
        "      X.append(context)\n",
        "      Y.append(s_to_i[ch])\n",
        "      context = context[1:] + [s_to_i[ch]]\n",
        "  return torch.tensor(X), torch.tensor(Y)\n",
        "\n",
        "random.shuffle(names)\n",
        "\n",
        "n1 = int(len(names) * 0.1)\n",
        "\n",
        "Xtr, Ytr = create_dataset(names[n1*2:]) # 80% train split\n",
        "Xte, Yte = create_dataset(names[:n1]) # 10% test\n",
        "Xval, Yval = create_dataset(names[n1:n1*2]) # 10% valid\n",
        "\n",
        "# precision spell\n",
        "avg_len = sum(len(name) + 1 for name in names) / len(names)\n",
        "actual = (len(Xtr) + len(Xte) + len(Xval)) / len(names)\n",
        "assert abs(avg_len - actual) < 1e-6, '\\nSize mismatch.\\nWhat did you break?'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train the Neural Network\n",
        "- Define model architecture with:\n",
        "  - Character embeddings\n",
        "  - Two hidden layers\n",
        "  - BatchNorm and ReLU activations\n",
        "- Train using cross-entropy loss and SGD\n",
        "- Monitor training and validation loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "510 parameters.\n",
            "step 0\n",
            "train loss: 3.62561\n",
            "valid loss: 3.65232\n",
            "avg train loss: 2.119587361955643\n"
          ]
        }
      ],
      "source": [
        "emb_dim = 8 # each character's embedding will be a vector of size (2).\n",
        "C = torch.randn(len(chars), emb_dim)\n",
        "emb = C[Xtr].view((Xtr.shape[0], -1))\n",
        "\n",
        "\n",
        "n_hidden = 100 #neuron count for hidden layers\n",
        "batch_size = 32\n",
        "\n",
        "layers = [\n",
        "    Linear(emb.shape[1], n_hidden), BatchNorm1d(n_hidden), Relu(),\n",
        "    Linear(n_hidden, 50), BatchNorm1d(50), Relu(),\n",
        "    Linear(50, len(chars))\n",
        "]\n",
        "\n",
        "parameters = []\n",
        "for layer in layers:\n",
        "  if hasattr(layer, 'params'):\n",
        "    parameters.extend(layer.params())\n",
        "parameters.append(C)\n",
        "\n",
        "for p in parameters:\n",
        "  p.requires_grad = True\n",
        "\n",
        "print(f'{len([p for param in parameters for p in param])} parameters.')\n",
        "\n",
        "losses = []\n",
        "for i in range(10000):\n",
        "\n",
        "  ib = torch.randint(0, emb.shape[0], (batch_size,)) # indices of the elements in the batch\n",
        "  #emb = C.view\n",
        "  Xb = C[Xtr[ib]].view(batch_size, -1)\n",
        "  Yb = Ytr[ib] # initialize batches\n",
        "\n",
        "# forward pass\n",
        "  for layer in layers:\n",
        "    Xb = layer(Xb)\n",
        "    layer.act = Xb\n",
        "# backward pass\n",
        "  loss = F.cross_entropy(Xb, Yb)\n",
        "  loss.backward()\n",
        "  losses.append(loss.item())\n",
        "# update\n",
        "  for p in parameters:\n",
        "    if i < 25000:\n",
        "        p.data += -0.01 * p.grad\n",
        "    else:\n",
        "        p.data += -0.001 * p.grad\n",
        "\n",
        "  for p in parameters:\n",
        "    p.grad = None\n",
        "  if i%10000 == 0:\n",
        "    print(f\"step {i}\\ntrain loss: {loss:.5f}\")\n",
        "    X = C[Xval].view(Xval.shape[0], -1)\n",
        "    Y = Yval # initialize batches\n",
        "    # forward pass\n",
        "    for layer in layers:\n",
        "        X = layer(X)\n",
        "    val_loss = F.cross_entropy(X, Y)\n",
        "    print(f\"valid loss: {val_loss:.5f}\")\n",
        "\n",
        "print(f\"avg train loss: {sum(losses) / len(losses)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## BatchNorm Folding for Faster Inference\n",
        "After training, remove BatchNorm layers by folding their behavior into the preceding Linear layer:\n",
        "- Adjust weights and biases using running mean/variance\n",
        "- Bake γ (scale) and β (shift) into Linear\n",
        "- Eliminate the BatchNorm layer entirely\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "# layers[3].weights = (layers[4].gamma * (layers[3].weights.T / torch.sqrt(layers[4].running_var + 1e-5))).T\n",
        "# layers[3].biases = (layers[4].gamma * (-layers[4].running_mean / torch.sqrt(layers[4].running_var + 1e-5))) + layers[4].beta\n",
        "\n",
        "# # Folding BatchNorm into the weights of the previous Linear layer\n",
        "# # Reference: https://forums.fast.ai/t/faster-inference-batch-normalization-folding/69161\n",
        "\n",
        "# layers.pop(4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ^^^^ what is this monstrosity???? let me explain.\n",
        "\n",
        "# we are effectively getting rid of the BatchNorm layer during inference.\n",
        "# this will allow us to decrease inference time.\n",
        "# we can do this because the BN operation after training is just a linear operation.\n",
        "# so what we do here is:\n",
        "#\n",
        "# Change each weight that follows a BN into:\n",
        "#     Weights divided by the standard deviation; \n",
        "#     Multiplied by the gamma, e.g. trainable scaling parameter of the BN layer.\n",
        "# We also need to account for beta, e.g. trainable shifting parameter, e.g. 'bias', of the BN layer.\n",
        "#\n",
        "# The result is a Linear layer with the BN baked into it. Pretty cool!\n",
        "# Then we just get rid of the BN layer. ^-^\n",
        "#\n",
        "#\n",
        "#   Now, let's do this to all BN layers:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[<__main__.Linear at 0x11cb91cd0>,\n",
              " <__main__.Relu at 0x11cbaae40>,\n",
              " <__main__.Linear at 0x11cb21d30>,\n",
              " <__main__.Relu at 0x11ce18170>,\n",
              " <__main__.Linear at 0x10fee85f0>]"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# modular :p\n",
        "for i, layer in enumerate(layers):\n",
        "    if hasattr(layer, 'running_mean'): # e.g. if the layer is BN\n",
        "        layers[i-1].weights = (layer.gamma * (layers[i-1].weights.T / torch.sqrt(layer.running_var + 1e-5))).T\n",
        "        layers[i-1].biases = (layer.gamma * (-layer.running_mean / torch.sqrt(layer.running_var + 1e-5))) + layer.beta\n",
        "layers = [layer for layer in layers if not hasattr(layer, 'running_mean')]\n",
        "layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluate Post-Folding Model\n",
        "- Run the model on test set after batchnorm has been removed\n",
        "- Check for consistent test loss to ensure folding didn't break anything\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "test loss: 1.7257429361343384\n"
          ]
        }
      ],
      "source": [
        "# Test results\n",
        "X = C[Xtr].view(Xtr.shape[0], -1)\n",
        "Y = Ytr # initialize batches\n",
        "# forward pass\n",
        "for layer in layers:\n",
        "    X = layer(X)\n",
        "# backward pass\n",
        "loss = F.cross_entropy(X, Y)\n",
        "print(f\"test loss: {loss}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generate New Names\n",
        "- Switch all layers to evaluation mode\n",
        "- Sample character-by-character using the trained model\n",
        "- Decode predicted names and map UTF-8 sigils back to Turkish characters\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "basmft\n",
            "emrat\n",
            "mehanap\n",
            "ezer\n",
            "hi̇subayn\n",
            "ui̇lami̇ye\n",
            "mehmet\n",
            "cerhanir\n",
            "özgen\n",
            "furay\n",
            "yuğazanee\n",
            "asşegürma\n",
            "aycm\n",
            "tul\n",
            "yus\n"
          ]
        }
      ],
      "source": [
        "# ---   INFERENCE   ---\n",
        "for layer in layers:\n",
        "  if hasattr(layer, 'training'):\n",
        "    layer.training = False\n",
        "\n",
        "while True:\n",
        "  name_count = int(input(\"How many names will be generated? (0 to exit): \"))\n",
        "  if name_count == 0:\n",
        "    break\n",
        "\n",
        "  for i in range(name_count):\n",
        "    context = [0] * block_size\n",
        "    name = ''\n",
        "    while True:\n",
        "      emb = C[context].view(-1, block_size * emb_dim)\n",
        "      for layer in layers:\n",
        "        emb = layer(emb)\n",
        "      pred = torch.multinomial(F.softmax(emb, dim=1), 1).item()\n",
        "      if i_to_s[pred] == '.':\n",
        "        break\n",
        "      context = context[1:] + [pred]\n",
        "      #print(context)\n",
        "      name += i_to_s[pred]\n",
        "\n",
        "    name = undo_utf8(name)\n",
        "    print(name)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
